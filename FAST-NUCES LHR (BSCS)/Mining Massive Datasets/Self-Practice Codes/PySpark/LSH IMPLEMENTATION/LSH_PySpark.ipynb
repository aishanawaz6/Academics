{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5am-41RqhZv",
        "outputId": "0f41e387-43e1-4000-c746-fe716868ef55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=ecbb498bb31b8c35009f4ac8283187283f908bf4d9d837aa09aca2b6898363f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u402-ga-2ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u402-ga-2ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Running on Colab\n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Importing Required Libraries\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create Spark session and ContextRun PySpark.\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\",\"4050\")\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.appName(\"DataFrame\").config('spark.ui.port', '4050').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "pTjpXKzzqyfM",
        "outputId": "b2504138-7b97-49a9-9893-b056228a14ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7d2dab13a410>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://44a25efa2148:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1: Shingling: Convert documents to sets"
      ],
      "metadata": {
        "id": "36vvucew_AU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "HugeDocuments = sc.parallelize([\n",
        "    'D1,Pretend this is a very huge document about data science and machine learning',\n",
        "    'D2,Pretend this is another very huge document about big data and its applications',\n",
        "    'D3,Mining of Massive Datasets is a course focusing on techniques for handling large-scale data'\n",
        "])\n",
        "\n",
        "K = 5  # Shingle length (9 is Ideal for large documents)\n",
        "buckets = 30 # Number of buckets for hashing (increased for better distribution)\n",
        "\n",
        "# Function to hash a shingle\n",
        "def hashIt(shingle):\n",
        "    hashObj = hashlib.sha256(shingle.encode())\n",
        "    hashed = int.from_bytes(hashObj.digest(), byteorder='big') % buckets\n",
        "    return hashed\n",
        "\n",
        "# Function to generate shingles from a document\n",
        "def getShingles(line):\n",
        "    global K\n",
        "    documentNumber, text = line.split(',', 1) #Split the string only at the first comma hence the ,1 this is to avoid in text commas split\n",
        "    text = text.lower()\n",
        "    setOfShingles = set()\n",
        "    for i in range(len(text) - K + 1):\n",
        "        shingle = text[i:i + K]\n",
        "        hashedShingle = hashIt(shingle)\n",
        "        setOfShingles.add(hashedShingle)\n",
        "    return documentNumber, setOfShingles\n",
        "\n",
        "# Generating the shingles\n",
        "shingles = HugeDocuments.map(lambda x: getShingles(x))\n",
        "\n",
        "# Removing duplicate shingles\n",
        "uniqueShingles = shingles.flatMap(lambda x: x[1]).distinct().collect()\n",
        "\n",
        "# Dictionary to map each shingle to an index\n",
        "shingleIndex = {shingle: i for i, shingle in enumerate(uniqueShingles)}\n",
        "\n",
        "# Map documents to shingle indices\n",
        "docShin = shingles.map(lambda x: (x[0], [shingleIndex[shingle] for shingle in x[1]]))\n",
        "\n",
        "# Create a sparse matrix\n",
        "sparseM = docShin.flatMapValues(lambda x: x).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x).sortByKey()\n",
        "\n",
        "# Convert sparse matrix to DataFrame\n",
        "dfData = sparseM.map(lambda x: (x[0][0], x[0][1], x[1])).toDF([\"Document\", \"Shingle\", \"Value\"])\n",
        "\n",
        "# Pivot the DataFrame to create the boolean matrix\n",
        "pivotedDf = dfData.groupby(\"Document\").pivot(\"Shingle\").agg({\"Value\": \"max\"}).fillna(0)\n",
        "\n",
        "# Show the boolean matrix\n",
        "pivotedDf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDq7By1L85vi",
        "outputId": "9ccb6588-9ea5-49a0-f68b-8d47f32fddc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|Document|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9| 10| 11| 12| 13| 14| 15| 16| 17| 18| 19| 20| 21| 22| 23| 24| 25| 26| 27| 28| 29|\n",
            "+--------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|      D1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  0|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  0|\n",
            "|      D3|  1|  1|  1|  1|  0|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
            "|      D2|  1|  0|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|  0|  1|  1|  0|  1|  1|  1|  1|  1|  1|  1|  0|  1|  1|  1|  1|\n",
            "+--------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Min-Hashing: Convert large sets to short signatures"
      ],
      "metadata": {
        "id": "104x0Lhf_I-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Efficient MinHashing using multiple hash functions\n",
        "numHashFunctions = 100\n",
        "\n",
        "# Generate hash functions\n",
        "def create_hash_functions(num_funcs, max_shingle):\n",
        "    def make_hash_function(seed):\n",
        "        random.seed(seed)\n",
        "        a = random.randint(1, max_shingle)\n",
        "        b = random.randint(0, max_shingle)\n",
        "        return lambda x: (a * x + b) % max_shingle\n",
        "    return [make_hash_function(i) for i in range(num_funcs)]\n",
        "\n",
        "hash_functions = create_hash_functions(numHashFunctions, len(uniqueShingles))\n",
        "\n",
        "# Function to update the signature matrix with min-hash values\n",
        "def updateSignature(row, hash_funcs):\n",
        "    document, shingleIndices = row\n",
        "    updatedSig = [float('inf')] * len(hash_funcs)\n",
        "    for shingleIndex in shingleIndices:\n",
        "        for i, hash_func in enumerate(hash_funcs):\n",
        "            hashed_val = hash_func(shingleIndex)\n",
        "            if hashed_val < updatedSig[i]:\n",
        "                updatedSig[i] = hashed_val\n",
        "    return document, updatedSig\n",
        "\n",
        "# Update signature matrix with min-hash\n",
        "signatureMatrix = docShin.map(lambda x: updateSignature(x, hash_functions))\n",
        "signatureDF = signatureMatrix.toDF([\"Document\", \"Signature\"])\n",
        "signatureDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2dadQySty6O",
        "outputId": "67e8e5b1-75bf-4d85-bc47-88aa1a977210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Document|Signature                                                                                                                                                                                                                                                                                                        |\n",
            "+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|D1      |[0, 3, 0, 0, 3, 8, 0, 0, 1, 4, 0, 12, 0, 0, 1, 0, 3, 0, 3, 1, 0, 1, 4, 24, 0, 0, 5, 0, 1, 2, 1, 0, 2, 0, 0, 4, 0, 1, 1, 0, 3, 0, 0, 1, 0, 1, 0, 2, 4, 2, 1, 0, 1, 7, 4, 0, 1, 1, 0, 0, 9, 1, 18, 14, 1, 1, 0, 0, 2, 1, 3, 0, 1, 0, 1, 3, 2, 0, 0, 4, 0, 0, 4, 1, 3, 1, 0, 3, 0, 0, 1, 0, 0, 27, 5, 2, 4, 0, 0, 0]|\n",
            "|D2      |[0, 3, 0, 0, 1, 8, 0, 0, 1, 4, 0, 12, 0, 0, 1, 0, 3, 1, 3, 1, 0, 1, 4, 24, 0, 0, 5, 0, 1, 2, 1, 0, 2, 2, 1, 4, 0, 1, 1, 0, 3, 0, 0, 1, 0, 1, 0, 2, 4, 2, 1, 0, 1, 7, 4, 0, 1, 1, 0, 0, 9, 1, 18, 14, 1, 1, 0, 0, 2, 1, 3, 0, 1, 0, 1, 3, 2, 0, 0, 4, 0, 0, 4, 1, 3, 1, 0, 3, 0, 0, 1, 0, 0, 27, 5, 2, 4, 0, 0, 0]|\n",
            "|D3      |[0, 3, 0, 0, 1, 8, 0, 0, 1, 4, 0, 12, 0, 0, 1, 0, 3, 0, 3, 1, 0, 1, 4, 24, 0, 0, 5, 0, 1, 2, 1, 0, 2, 0, 0, 4, 0, 1, 1, 0, 3, 0, 0, 1, 0, 1, 0, 2, 4, 2, 1, 0, 1, 7, 4, 0, 1, 1, 0, 0, 9, 1, 18, 14, 1, 1, 0, 0, 2, 1, 3, 1, 1, 0, 1, 3, 2, 0, 0, 4, 0, 0, 4, 1, 3, 0, 0, 3, 0, 0, 0, 0, 0, 27, 5, 2, 4, 0, 0, 0]|\n",
            "+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Locality-Sensitive Hashing:"
      ],
      "metadata": {
        "id": "he9CVCfi_PkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of bands and rows per band\n",
        "num_bands = 10\n",
        "rows_per_band = len(hash_functions) // num_bands\n",
        "\n",
        "# Function to hash a band\n",
        "def hash_band(band):\n",
        "    return hashlib.md5(band.encode()).hexdigest()\n",
        "\n",
        "# Create candidate pairs using LSH\n",
        "def lsh(signature, num_bands, rows_per_band):\n",
        "    candidates = set()\n",
        "    for band_idx in range(num_bands):\n",
        "        band = signature[band_idx * rows_per_band: (band_idx + 1) * rows_per_band]\n",
        "        band_str = ','.join(map(str, band))\n",
        "        band_hash = hash_band(band_str)\n",
        "        candidates.add((band_hash, band_idx))\n",
        "    return candidates\n",
        "\n",
        "# Apply LSH to the signature matrix\n",
        "lsh_rdd = signatureMatrix.flatMapValues(lambda x: lsh(x, num_bands, rows_per_band))\n",
        "\n",
        "# Group by band hash to find candidate pairs\n",
        "candidate_pairs = lsh_rdd.map(lambda x: (x[1], x[0])).groupByKey().flatMap(lambda x: combinations(x[1], 2)).distinct()\n",
        "\n",
        "# Collect candidate pairs\n",
        "candidate_pairs_collected = candidate_pairs.collect()\n",
        "\n",
        "# Calculate Jaccard similarity for candidate pairs\n",
        "def jaccard_similarity(pair, signature_matrix):\n",
        "    doc1, doc2 = pair\n",
        "    sig1 = signature_matrix[doc1]\n",
        "    sig2 = signature_matrix[doc2]\n",
        "    intersection = 0\n",
        "    for i in range(len(sig1)):\n",
        "        if sig1[i] == sig2[i]:\n",
        "            intersection += 1\n",
        "    union = len(sig1) #length of either signature vector (since both vectors are of the same length).\n",
        "    return intersection / union\n",
        "\n",
        "# Convert signature matrix to a dictionary for fast access\n",
        "signature_matrix_dict = {row[0]: row[1] for row in signatureMatrix.collect()}\n",
        "\n",
        "# Calculate and filter pairs with high similarity\n",
        "threshold = 0.95\n",
        "similar_pairs = [(pair, jaccard_similarity(pair, signature_matrix_dict)) for pair in candidate_pairs_collected if jaccard_similarity(pair, signature_matrix_dict) >= threshold]\n",
        "\n",
        "# Display similar pairs\n",
        "print(\"Documents with similarity greater than 95 percent:\", similar_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HFH4_r_-oGc",
        "outputId": "9a6061fa-3ee4-4d32-f8c2-c1b126a13526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents with similarity greater than 95 percent: [(('D1', 'D3'), 0.96), (('D1', 'D2'), 0.96)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UhJQFxXy-yOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}