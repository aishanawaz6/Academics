{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p145P4trLd_",
        "outputId": "21b63d84-f2c8-4a89-a1f2-2f69a50efa47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=51f65a301a5cc2eae6ef174bf2b6ec8393cf67eb6faa88bf305febf39cd1be7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u402-ga-2ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u402-ga-2ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Running on Colab\n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Importing Required Libraries\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create Spark session and ContextRun PySpark.\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\",\"4050\")\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.appName(\"DataFrame\").config('spark.ui.port', '4050').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "TiyjT8pYrU0t",
        "outputId": "6aa801f8-c0ed-4368-8a54-87a11a9e93f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79308ee328c0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f006e1d9b344:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Davies-Bouldin index (DBI)**"
      ],
      "metadata": {
        "id": "7yXuYjEErgvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getPointInfo(line):\n",
        "  points,clusterID=line.split(\"), \")\n",
        "  pointX,pointY=points.split(',')\n",
        "  pointX=float(pointX.replace('(',''))\n",
        "  pointY=float(pointY)\n",
        "\n",
        "  return clusterID,(pointX,pointY,1)\n",
        "\n",
        "clustersData=sc.textFile(\"F1.txt\").map(lambda line: getPointInfo(line))\n",
        "clustersData.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YaNajGUrXGi",
        "outputId": "fd4ba020-df49-4344-fd75-fa178d78914c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('C1', (1.0, 2.0, 1)),\n",
              " ('C2', (5.0, 5.0, 1)),\n",
              " ('C1', (2.0, 2.0, 1)),\n",
              " ('C3', (11.0, 11.0, 1))]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "centroids=clustersData.reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1],x[2]+y[2])).mapValues(lambda x:(x[0]/x[2],x[1]/x[2]))\n",
        "centroids.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgEblgFttvz_",
        "outputId": "9d1fc844-5ddc-47a3-9e92-b77e21807ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('C1', (1.5, 2.0)), ('C2', (5.0, 5.0)), ('C3', (11.0, 11.0))]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "centroidsPointsDist=centroids.join(clustersData).mapValues(lambda x: (((x[0][0]-x[1][0])**2+(x[0][1]-x[1][1])**2),1)).reduceByKey(lambda x,y:((np.sqrt(x[0])+np.sqrt(y[0])),(x[1]+y[1])))\n",
        "centroidsPointsDist=centroidsPointsDist.mapValues(lambda x: x[0]/x[1])\n",
        "centroidsPointsDist.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdZ0rKKO0oy1",
        "outputId": "feacc759-9eef-4ee7-a82f-c7dffd41003c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('C1', 0.5), ('C2', 0.0), ('C3', 0.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "centoidsDict=dict(centroids.collect())\n",
        "centroidsPointsDistDict=dict(centroidsPointsDist.collect())\n",
        "results={}\n",
        "for key,value in centoidsDict.items():\n",
        "  for key2,value2 in centoidsDict.items():\n",
        "    if(key!=key2 and (str(key)+str(key2) not in results.keys()) and (str(key2)+str(key) not in results.keys())):\n",
        "      newPair=str(key)+str(key2)\n",
        "      euclideanDistanceBetweenCentroid = np.sqrt((value[0]-value2[0])**2+(value[1]-value2[1])**2)\n",
        "      maxDist=centroidsPointsDistDict[key] if centroidsPointsDistDict[key]>centroidsPointsDistDict[key2] else centroidsPointsDistDict[key2]\n",
        "\n",
        "      results[newPair]=euclideanDistanceBetweenCentroid/maxDist if maxDist > 0 else 0"
      ],
      "metadata": {
        "id": "jZPKqj361A9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNyKzrNe2aYD",
        "outputId": "d711dc99-0fb5-45b2-9b13-bebd0d0e3c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C1C2': 9.219544457292887, 'C1C3': 26.1725046566048, 'C2C3': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sumVal,countVal=0,0\n",
        "for key, value in results.items():\n",
        "  sumVal=sumVal+value\n",
        "  countVal=countVal+1\n",
        "dbi=sumVal/countVal if countVal>0 else 0\n",
        "print(\"DBI=\",dbi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nAtNMN63NbZ",
        "outputId": "f1b4ba55-a4da-46a4-aff1-00af53ca67ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DBI= 11.79734970463256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ALL TOGETHER"
      ],
      "metadata": {
        "id": "zlxyzL429IPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def getPointInfo(line):\n",
        "  points,clusterID=line.split(\"), \")\n",
        "  pointX,pointY=points.split(',')\n",
        "  pointX=float(pointX.replace('(',''))\n",
        "  pointY=float(pointY)\n",
        "\n",
        "  return clusterID,(pointX,pointY,1)\n",
        "\n",
        "clustersData=sc.textFile(\"F1.txt\").map(lambda line: getPointInfo(line))\n",
        "clustersData.collect()\n",
        "\n",
        "\n",
        "centroids=clustersData.reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1],x[2]+y[2])).mapValues(lambda x:(x[0]/x[2],x[1]/x[2]))\n",
        "\n",
        "\n",
        "centroidsPointsDist=centroids.join(clustersData).mapValues(lambda x: (((x[0][0]-x[1][0])**2+(x[0][1]-x[1][1])**2),1)).reduceByKey(lambda x,y:((np.sqrt(x[0])+np.sqrt(y[0])),(x[1]+y[1])))\n",
        "centroidsPointsDist=centroidsPointsDist.mapValues(lambda x: x[0]/x[1])\n",
        "\n",
        "\n",
        "centoidsDict=dict(centroids.collect())\n",
        "centroidsPointsDistDict=dict(centroidsPointsDist.collect())\n",
        "results={}\n",
        "for key,value in centoidsDict.items():\n",
        "  for key2,value2 in centoidsDict.items():\n",
        "    if(key!=key2 and (str(key)+str(key2) not in results.keys()) and (str(key2)+str(key) not in results.keys())):\n",
        "      newPair=str(key)+str(key2)\n",
        "      euclideanDistanceBetweenCentroid = np.sqrt((value[0]-value2[0])**2+(value[1]-value2[1])**2)\n",
        "      maxDist=centroidsPointsDistDict[key] if centroidsPointsDistDict[key]>centroidsPointsDistDict[key2] else centroidsPointsDistDict[key2]\n",
        "\n",
        "      results[newPair]=euclideanDistanceBetweenCentroid/maxDist if maxDist > 0 else 0\n",
        "\n",
        "sumVal,countVal=0,0\n",
        "for key, value in results.items():\n",
        "  sumVal=sumVal+value\n",
        "  countVal=countVal+1\n",
        "dbi=sumVal/countVal if countVal>0 else 0\n",
        "print(\"DBI=\",dbi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9ww7Jwc4826",
        "outputId": "920a07f2-f1b6-4729-c41c-91a5c7e8561e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DBI= 11.79734970463256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAME CODE USING DATAFRAMES**"
      ],
      "metadata": {
        "id": "DgpYGEEu-cSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, col, udf, sum as spark_sum, count as spark_count\n",
        "from pyspark.sql.types import StructType, StructField, FloatType, StringType, IntegerType\n",
        "import numpy as np\n",
        "\n",
        "# Define schema for input data\n",
        "schema = StructType([\n",
        "    StructField(\"points\", StringType(), True),\n",
        "    StructField(\"clusterID\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Read data into DataFrame\n",
        "data = spark.read.option(\"delimiter\", \"), \").schema(schema).csv(\"F1.txt\")\n",
        "\n",
        "# Extract x, y coordinates from points and create new DataFrame\n",
        "points_df = data.withColumn(\"pointX\", split(col(\"points\"), \",\").getItem(0).substr(2, 100).cast(FloatType())) \\\n",
        "                .withColumn(\"pointY\", split(col(\"points\"), \",\").getItem(1).cast(FloatType()))\n",
        "\n",
        "# Calculate centroids\n",
        "centroids = points_df.groupBy(\"clusterID\") \\\n",
        "                     .agg(spark_sum(\"pointX\").alias(\"sumX\"), spark_sum(\"pointY\").alias(\"sumY\"), spark_count(\"*\").alias(\"count\")) \\\n",
        "                     .withColumn(\"centroidX\", col(\"sumX\") / col(\"count\")) \\\n",
        "                     .withColumn(\"centroidY\", col(\"sumY\") / col(\"count\"))\n",
        "\n",
        "# Calculate distances from points to centroids\n",
        "joined_df = points_df.join(centroids, \"clusterID\") \\\n",
        "                     .withColumn(\"distance\", ((col(\"centroidX\") - col(\"pointX\"))**2 + (col(\"centroidY\") - col(\"pointY\"))**2)**0.5)\n",
        "\n",
        "# Calculate average distance to centroid for each cluster\n",
        "average_distances = joined_df.groupBy(\"clusterID\") \\\n",
        "                             .agg(spark_sum(\"distance\").alias(\"sumDistance\"), spark_count(\"*\").alias(\"count\")) \\\n",
        "                             .withColumn(\"avgDistance\", col(\"sumDistance\") / col(\"count\"))\n",
        "\n",
        "# Convert centroids and average distances to dictionaries\n",
        "centroids_dict = {row['clusterID']: (row['centroidX'], row['centroidY']) for row in centroids.collect()}\n",
        "average_distances_dict = {row['clusterID']: row['avgDistance'] for row in average_distances.collect()}\n",
        "\n",
        "# Calculate DBI\n",
        "results = {}\n",
        "for key, value in centroids_dict.items():\n",
        "    for key2, value2 in centroids_dict.items():\n",
        "        if key != key2 and (str(key) + str(key2) not in results) and (str(key2) + str(key) not in results):\n",
        "            euclidean_distance = np.sqrt((value[0] - value2[0])**2 + (value[1] - value2[1])**2)\n",
        "            max_dist = average_distances_dict[key] if average_distances_dict[key]>average_distances_dict[key2] else average_distances_dict[key2]\n",
        "            results[str(key) + str(key2)] = euclidean_distance / max_dist if max_dist > 0 else 0\n",
        "\n",
        "# Initialize variables for DBI calculation\n",
        "sum_val = 0\n",
        "count_val = 0\n",
        "\n",
        "# Calculate DBI using results dictionary\n",
        "for value in results.values():\n",
        "    sum_val += value\n",
        "    count_val += 1\n",
        "\n",
        "# Avoid division by zero error\n",
        "dbi = sum_val / count_val if count_val > 0 else 0\n",
        "\n",
        "# Print DBI value\n",
        "print(\"DBI =\", dbi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A41lSVVM_dEO",
        "outputId": "1e6e279d-6d12-4d2c-fe91-86b8438ab048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DBI = 11.79734970463256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HT-9gaixAkjc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}