{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Aisha Muhammad Nawaz  L200921**  \n",
        "PySpark Class Activity\n",
        "8A BSCS MMD  \n",
        "23rd April 2024\n",
        "\n",
        "Instructions:\n",
        "2. Write efficient Spark code for creating K-shingles given a huge document and K as input.\n",
        "3. Write an efficient SPARK code for Minhashing (uses the logic of hash functions as shown in the uploaded slide). The map reduce code is given in slides.\n"
      ],
      "metadata": {
        "id": "s5RFSsH6-Obr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I62evYoa99rn",
        "outputId": "7215ab43-3093-44b7-98ae-731afcbc26f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=46fb6e6cffc7c6d40c7dc511d904e7bcd1a55447f58431d4859af001a126bf37\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 121752 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u402-ga-2ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u402-ga-2ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# #Running on Colab\n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye3fpH5gB4r_",
        "outputId": "56e620e6-89c7-457f-edd9-85b9c59299b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [1\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [C\u001b[0m\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [814 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,748 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,251 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,077 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,032 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,369 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,333 kB]\n",
            "Fetched 11.9 MB in 3s (4,386 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import the libraries we will need\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create Spark session and ContextRun PySpark.\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\",\"4050\")\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.appName(\"DataFrame\").config('spark.ui.port', '4050').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "hb9bqHVT9-LJ",
        "outputId": "5474188a-4c87-412e-d3c2-1aa82347dbba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ef6ffb1a560>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://97f0daf8320a:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2: Write efficient Spark code for creating K-shingles given a huge document and K as input.\n",
        "import hashlib\n",
        "HugeDocument = sc.parallelize(['D1,Pretend it is docu','D2,Pretend it no docu'])\n",
        "K=9\n",
        "d=0\n",
        "\n",
        "def hashIt(shingle):\n",
        "    # Hash the shingle to 4 bytes\n",
        "    buckets = 8\n",
        "    hashObj = hashlib.sha256(shingle.encode())\n",
        "    hashed = int.from_bytes(hashObj.digest(), byteorder='big') % buckets\n",
        "    return hashed\n",
        "def getShingles(line):\n",
        "    global K\n",
        "    documentNumber, text = line.split(',')\n",
        "    text = text.lower()\n",
        "    setOfShingles = set()\n",
        "    for i in range(len(text) - K + 1):\n",
        "        shingle = text[i:i+K]\n",
        "        hashedShingle = hashIt(shingle)\n",
        "        setOfShingles.add(hashedShingle)\n",
        "    return documentNumber, setOfShingles\n",
        "\n",
        "shingles=HugeDocument.map(lambda x: getShingles(x))\n",
        "\n",
        "# Removing Duplicate Shingles\n",
        "uniqueShingles = shingles.flatMap(lambda x: x[1]).distinct().collect()\n",
        "\n",
        "# Getting Boolean Matrix ---->\n",
        "\n",
        "shingleIndex = {shingle: i for i, shingle in enumerate(uniqueShingles)} # Dictionary to map each shingle to an index\n",
        "docShin = shingles.map(lambda x: (x[0], [shingleIndex[shingle] for shingle in x[1]])) # List of document and shingle pairs\n",
        "sparseM = docShin.flatMapValues(lambda x: x).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x).sortByKey()\n",
        "dfData = sparseM.map(lambda x: (x[0][0], x[0][1], x[1])).toDF([\"Document\", \"Shingle\", \"Value\"]) # Convert sparseM RDD to DataFrame\n",
        "pivotedDf = dfData.groupby(\"Document\").pivot(\"Shingle\").agg({\"Value\": \"max\"})\n",
        "pivotedDf = pivotedDf.fillna(0) # Null values filled with 0\n",
        "pivotedDf.show()"
      ],
      "metadata": {
        "id": "trVkJaVo-Bjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046e0167-5667-4ab5-af41-307c94f106e0"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+---+---+---+---+---+---+---+\n",
            "|Document|  0|  1|  2|  3|  4|  5|  6|  7|\n",
            "+--------+---+---+---+---+---+---+---+---+\n",
            "|      D1|  1|  1|  1|  0|  1|  1|  1|  1|\n",
            "|      D2|  1|  0|  1|  1|  1|  1|  1|  0|\n",
            "+--------+---+---+---+---+---+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3:  Write an efficient SPARK code for Minhashing\n",
        "import random\n",
        "\n",
        "# 100 random permutations of the rows\n",
        "KPermutations = [random.sample(range(len(uniqueShingles)), len(uniqueShingles)) for _ in range(100)]\n",
        "\n",
        "def updateSignature(row, permutations):\n",
        "    document, shingleIndices = row\n",
        "    updatedSig = [float('inf')] * len(permutations)\n",
        "    for shingleIndex in shingleIndices:\n",
        "        for i, perm in enumerate(permutations):\n",
        "            if perm.index(shingleIndex) < updatedSig[i]:\n",
        "                updatedSig[i] = perm.index(shingleIndex)\n",
        "    return document, updatedSig\n",
        "\n",
        "# Update signature matrix with min-hash\n",
        "signatureMatrix = docShin.map(lambda x: updateSignature(x, KPermutations))\n",
        "signatureDF = signatureMatrix.toDF([\"Document\", \"Signature\"])\n",
        "signatureDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KWkKNs3bB43",
        "outputId": "b1f45db6-178f-48ea-854f-16e96ebacf93"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Document|Signature                                                                                                                                                                                                                                                                                                   |\n",
            "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|D1      |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0]|\n",
            "|D2      |[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]|\n",
            "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HFdk8U6BY8kc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}