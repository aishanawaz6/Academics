{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR_WMZBULJSh",
        "outputId": "745bba04-48c8-4133-966c-b9b079dcca83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=51434d7aee17c9e69f7266858ac0c8a2e9f7811d66f065a13cfa7fcc3813d10d\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u402-ga-2ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u402-ga-2ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# #Running on Colab\n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import the libraries we will need\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create Spark session and ContextRun PySpark.\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\",\"4050\")\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.appName(\"DataFrame\").config('spark.ui.port', '4050').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "2LdLvbtBLQgN",
        "outputId": "d638a6b0-c6a5-4fb7-9ff6-13efebb5877b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78cee2aadba0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9db9d4ff9faa:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = sc.parallelize([(0,2), (3,4), (5,6), (20,8), (2,2), (3,4), (6,6), (8,8),(4,2), (12,4), (15,6), (7,8)], 3)\n",
        "print('Number of partitions:{} '.format(rdd1.getNumPartitions()))\n",
        "print('Partitioner: {}'.format(rdd1.partitioner))\n",
        "print('Partitions structure: {}'.format(rdd1.glom().collect()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7DC0nLdLUIq",
        "outputId": "ac7ebf8e-73f9-4768-c4de-eefee9bf937f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions:3 \n",
            "Partitioner: None\n",
            "Partitions structure: [[(0, 2), (3, 4), (5, 6), (20, 8)], [(2, 2), (3, 4), (6, 6), (8, 8)], [(4, 2), (12, 4), (15, 6), (7, 8)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([(0,2), (3,4), (5,6), (20,8), (2,2), (3,4), (6,6), (8,8),\n",
        "(4,2), (12,4), (15,6), (7,8)], 3)\n",
        "rdd1 = rdd.partitionBy(5)\n",
        "print('Number of partitions: {}'.format(rdd1.getNumPartitions()))\n",
        "print('Partitioner: {}'.format(rdd1.partitioner))\n",
        "print('Partitions structure: {}'.format(rdd1.glom().collect()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk-Y7Y_5L3bo",
        "outputId": "28381005-d6dd-4ac3-f935-1ecdc24689a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions: 5\n",
            "Partitioner: <pyspark.rdd.Partitioner object at 0x78cf0414ab00>\n",
            "Partitions structure: [[(0, 2), (5, 6), (20, 8), (15, 6)], [(6, 6)], [(2, 2), (12, 4), (7, 8)], [(3, 4), (3, 4), (8, 8)], [(4, 2)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custome Partitioner\n",
        "def partFunc(k):\n",
        "  if(k % 2 == 0 ):\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "rdd = sc.parallelize([(0,2), (3,4), (5,6), (20,8), (2,2), (3,4),\n",
        "(6,6), (8,8), (4,2), (12,4), (15,6), (7,8)])\n",
        "rdd1 = rdd.partitionBy(2,lambda x: partFunc(x)).persist()\n",
        "print('Number of partitions: {}'.format(rdd1.getNumPartitions()))\n",
        "print('Partitioner: {}'.format(rdd1.partitioner))\n",
        "print('Partitions structure: {}'.format(rdd1.glom().collect()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDZPyzr8NAe4",
        "outputId": "e124930b-2ced-4a3d-f34d-09a34958db60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions: 2\n",
            "Partitioner: <pyspark.rdd.Partitioner object at 0x78cee2aad900>\n",
            "Partitions structure: [[(0, 2), (20, 8), (2, 2), (6, 6), (8, 8), (4, 2), (12, 4)], [(3, 4), (5, 6), (3, 4), (15, 6), (7, 8)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accumulators  & Boardcast\n",
        "counter = sc.accumulator(0)\n",
        "\n",
        "# Define broadcast variable\n",
        "data_to_broadcast = [1, 2, 3, 4, 5]\n",
        "broadcast_var = sc.broadcast(data_to_broadcast)\n",
        "\n",
        "# Sample RDD to demonstrate accumulator and broadcast variable usage\n",
        "rdd = sc.parallelize(range(10))\n",
        "\n",
        "# Function to increment accumulator and use broadcast variable\n",
        "def process_element(x):\n",
        "    global counter\n",
        "    global broadcast_var\n",
        "\n",
        "    # Increment accumulator\n",
        "    counter += 1\n",
        "\n",
        "    # Access and use broadcast variable\n",
        "    broadcast_data = broadcast_var.value\n",
        "    return x * broadcast_data[1]\n",
        "\n",
        "# Apply function to each element in RDD\n",
        "result = rdd.map(process_element)\n",
        "\n",
        "# Collect and print result\n",
        "print(result.collect())\n",
        "\n",
        "# Print accumulator value\n",
        "print(\"Accumulator value:\", counter.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Yci895KOidK",
        "outputId": "d1ca7ba3-adde-4e9d-c247-ea93f4103585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
            "Accumulator value: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word co-occurence in Spark\n",
        "# myFile.txt is as follows:\n",
        "# great first try super boy\n",
        "# first try good\n",
        "# super boy won first try\n",
        "\n",
        "input = sc.textFile(\"myFile.txt\")\n",
        "co = input.map(lambda x:x.split(\" \"))\n",
        "co.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUozPlXTQtf6",
        "outputId": "e1b0afa7-33bf-4e4f-e479-3af7ac96688f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['great', 'first', 'try', 'super', 'boy'],\n",
              " ['first', 'try', 'good'],\n",
              " ['super', 'boy', 'won', 'first', 'try']]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = sc.textFile(\"myFile.txt\")\n",
        "co = input.flatMap(lambda x:x.split(\" \"))\n",
        "co.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J8GmZZ5RhVj",
        "outputId": "05cb5044-314a-447c-aa38-656a9fde449f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['great',\n",
              " 'first',\n",
              " 'try',\n",
              " 'super',\n",
              " 'boy',\n",
              " 'first',\n",
              " 'try',\n",
              " 'good',\n",
              " 'super',\n",
              " 'boy',\n",
              " 'won',\n",
              " 'first',\n",
              " 'try']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = sc.textFile(\"myFile.txt\")\n",
        "co = input.map(lambda x:x.split(\" \"))\n",
        "\n",
        "def func(line):\n",
        "  value =[]\n",
        "  for i in range(len(line)-1):\n",
        "    for j in range(i+1,len(line)):\n",
        "      value.append(((line[i],line[j]),1))\n",
        "  return value\n",
        "\n",
        "co2 = co.flatMap(func)\n",
        "co3 =co2.reduceByKey(lambda x,y:x+y)\n",
        "co3.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l97m7JLiRzoh",
        "outputId": "257225da-f4e2-4830-dc9e-01ec0a343eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('great', 'first'), 1),\n",
              " (('try', 'super'), 1),\n",
              " (('try', 'boy'), 1),\n",
              " (('super', 'boy'), 2),\n",
              " (('try', 'good'), 1),\n",
              " (('super', 'try'), 1),\n",
              " (('boy', 'try'), 1),\n",
              " (('won', 'first'), 1),\n",
              " (('great', 'try'), 1),\n",
              " (('great', 'super'), 1),\n",
              " (('great', 'boy'), 1),\n",
              " (('first', 'try'), 3),\n",
              " (('first', 'super'), 1),\n",
              " (('first', 'boy'), 1),\n",
              " (('first', 'good'), 1),\n",
              " (('super', 'won'), 1),\n",
              " (('super', 'first'), 1),\n",
              " (('boy', 'won'), 1),\n",
              " (('boy', 'first'), 1),\n",
              " (('won', 'try'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in map reduce\n",
        "from mrjob.job import MRJob\n",
        "\n",
        "class WordPairsMRJob(MRJob):\n",
        "    def mapper(self, _, line):\n",
        "        words = line.split(\" \")\n",
        "        # Emit all pairs of words in the line\n",
        "        for i in range(len(words) - 1):\n",
        "            for j in range(i + 1, len(words)):\n",
        "                # Ensure that the pair is emitted\n",
        "                yield tuple(([words[i], words[j]])), 1\n",
        "\n",
        "    def reducer(self, key, values):\n",
        "        # Sum up the counts for each pair\n",
        "        yield key, sum(values)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    WordPairsMRJob.run()\n"
      ],
      "metadata": {
        "id": "JItt2zktTlGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution to same pair different order issue (E.g boy try and try boy are same pairs)\n",
        "# IDEA: Generate pair so that words are in sorted order\n",
        "\n",
        "input = sc.textFile(\"myFile.txt\")\n",
        "co = input.map(lambda x:x.split(\" \"))\n",
        "\n",
        "def func(line):\n",
        "  value =[]\n",
        "  for i in range(len(line)-1):\n",
        "    for j in range(i+1,len(line)):\n",
        "      if(line[i]<=line[j]):\n",
        "        value.append(((line[i],line[j]),1))\n",
        "      else:\n",
        "        value.append(((line[j],line[i]),1))\n",
        "  return value\n",
        "\n",
        "co2 = co.flatMap(func)\n",
        "co3 =co2.reduceByKey(lambda x,y:x+y)\n",
        "co3.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H7JwfiuSLv4",
        "outputId": "5787473e-aafb-4f19-b5e4-6c45ddb19c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('first', 'great'), 1),\n",
              " (('super', 'try'), 2),\n",
              " (('boy', 'try'), 2),\n",
              " (('boy', 'super'), 2),\n",
              " (('good', 'try'), 1),\n",
              " (('first', 'won'), 1),\n",
              " (('great', 'try'), 1),\n",
              " (('great', 'super'), 1),\n",
              " (('boy', 'great'), 1),\n",
              " (('first', 'try'), 3),\n",
              " (('first', 'super'), 2),\n",
              " (('boy', 'first'), 2),\n",
              " (('first', 'good'), 1),\n",
              " (('super', 'won'), 1),\n",
              " (('boy', 'won'), 1),\n",
              " (('try', 'won'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pi calculation problem\n",
        "import random\n",
        "partitions = 2\n",
        "n = 100000 * partitions\n",
        "\n",
        "def func(_):\n",
        "    x = random.random() * 2 - 1\n",
        "    y = random.random() * 2 - 1 #Range between -1 to 1\n",
        "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
        "\n",
        "sample = spark.sparkContext.parallelize(range(1, n + 1), partitions)\n",
        "count = sample.map(func).reduce(lambda x, y: x + y)\n",
        "print(\"Pi is roughly %f\" % (4.0 * count / n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZWlj4ITS2Xa",
        "outputId": "d4adec5d-c69b-4c80-b7c0-d8d8fcaf0584"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi is roughly 3.143360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PI PROBLEM IN MAP REDUCE\n",
        "from mrjob.job import MRJob\n",
        "import random\n",
        "\n",
        "class PiCalculationMRJob(MRJob):\n",
        "\n",
        "    def mapper(self, _, __):\n",
        "        partitions = 2\n",
        "        n = 100000 * partitions\n",
        "\n",
        "        # Generate random points and determine if they fall inside the unit circle\n",
        "        for _ in range(n):\n",
        "            x = random.random() * 2 - 1\n",
        "            y = random.random() * 2 - 1\n",
        "            yield None, (1 if x ** 2 + y ** 2 <= 1 else 0)\n",
        "\n",
        "    def combiner(self, _, counts):\n",
        "        # Local aggregation of counts of points inside the unit circle\n",
        "        total_points = 0\n",
        "        points_inside_circle = 0\n",
        "        for count in counts:\n",
        "            total_points += 1\n",
        "            points_inside_circle += count\n",
        "        yield None, (total_points, points_inside_circle)\n",
        "\n",
        "    def reducer(self, _, counts):\n",
        "        # Aggregate counts of points inside the unit circle received from combiners\n",
        "        total_points = 0\n",
        "        points_inside_circle = 0\n",
        "        for count in counts:\n",
        "            total_points += count[0]\n",
        "            points_inside_circle += count[1]\n",
        "        yield None, 4.0 * points_inside_circle / total_points\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    PiCalculationMRJob.run()\n"
      ],
      "metadata": {
        "id": "aUMLIZfFUZa0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}