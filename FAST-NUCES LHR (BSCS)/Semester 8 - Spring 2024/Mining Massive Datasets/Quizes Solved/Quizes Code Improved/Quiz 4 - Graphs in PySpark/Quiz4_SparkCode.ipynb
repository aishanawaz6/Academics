{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "whGHEMOyHWe5",
        "outputId": "b37f68e6-337f-4ae4-fdd3-45bf0bd6d4ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=ef34d81303fcc18326e1bcf553d6dc1d0cd1b859cd8d45dc35c021871269f1ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u402-ga-2ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u402-ga-2ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Running on Colab\n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Importing Required Libraries\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create Spark session and ContextRun PySpark.\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\",\"4050\")\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.appName(\"DataFrame\").config('spark.ui.port', '4050').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "keZxSfyJHcoI",
        "outputId": "315c117e-a1dd-49e9-f35d-fe1884fc1ffe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7cc85396b850>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://125f9ef93979:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Write a Spark code that read a directed graph as adjacency\n",
        "list from a textFile and convert it to undirected graph.\n"
      ],
      "metadata": {
        "id": "kQESw9LGHg34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = sc.textFile('input.txt').map(lambda x: (x.split('->')[0],x.split('->')[1].split(',')))\n",
        "inp1=inp.flatMap(lambda x: [(x[0],y) for y in x[1]])\n",
        "inp2=inp.flatMap(lambda x: [(y,x[0]) for y in x[1]])\n",
        "inpAll=inp1.union(inp2)\n",
        "results=inpAll.groupByKey()\n",
        "for key, value in results.collect():\n",
        "    print(key, list(value))"
      ],
      "metadata": {
        "id": "UDhdvwExNE8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03345c24-0e58-4968-8824-d41c79e5f723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 ['3', '2']\n",
            "3 ['1', '4']\n",
            "1 ['2', '3']\n",
            "2 ['4', '1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try Before Final Exam"
      ],
      "metadata": {
        "id": "7DS1M27DaPn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directedGraph=sc.textFile('input.txt')\n",
        "vertexEdges=directedGraph.map(lambda line: (line.split('->')[0],line.split('->')[1].split(',')) )\n",
        "vertexEdgesAll=vertexEdges.flatMap(lambda value:[(val,value[0]) for val in value[1]])\n",
        "vertexEdgesAll2=vertexEdges.flatMap(lambda value:[(value[0],val) for val in value[1]])\n",
        "res=vertexEdgesAll.union(vertexEdgesAll2).groupByKey().collect()\n",
        "for key, value in res:\n",
        "    print(key, list(value))"
      ],
      "metadata": {
        "id": "PdTnuFdhjICF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a6008fb9-3e13-463d-8e48-8de62ea17fc4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 <pyspark.resultiterable.ResultIterable object at 0x7cc85396bd00>\n",
            "3 <pyspark.resultiterable.ResultIterable object at 0x7cc85396b6d0>\n",
            "1 <pyspark.resultiterable.ResultIterable object at 0x7cc853898d60>\n",
            "2 <pyspark.resultiterable.ResultIterable object at 0x7cc85389b490>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the directed graph from the input file\n",
        "directedGraph = sc.textFile('input.txt')\n",
        "\n",
        "# Parse each line into a tuple of (vertex, [edges])\n",
        "vertexEdges = directedGraph.map(lambda line: (line.split('->')[0].strip(), line.split('->')[1].strip().split(',')))\n",
        "\n",
        "# Create bidirectional (vertex, edge) pairs\n",
        "bidirectionalEdges = vertexEdges.flatMap(lambda value: [(value[0], val.strip()) for val in value[1]] + [(val.strip(), value[0]) for val in value[1]])\n",
        "# bidirectionalEdges.collect()\n",
        "# Group by vertex and aggregate the edges\n",
        "undirectedGraph = bidirectionalEdges.groupByKey().mapValues(lambda vals: sorted(set(vals))).collect()\n",
        "\n",
        "# Print the results\n",
        "for key, value in undirectedGraph:\n",
        "    print(f\"{key}-> {', '.join(value)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gZjqc_bsaMR5",
        "outputId": "d637359c-29da-425b-f176-4619da94bafe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-> 2, 3\n",
            "4-> 2, 3\n",
            "2-> 1, 4\n",
            "3-> 1, 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2JErgm8jci25"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}